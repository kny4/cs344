{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a5349b1",
   "metadata": {},
   "source": [
    "# Building a Song Recommender  \n",
    "by Braden Weber (blw22) and Kelsey Yen (kny4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94f5e68",
   "metadata": {},
   "source": [
    "## Goal  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c7875",
   "metadata": {},
   "source": [
    "The goal of our project was to create a song recommender. Using the fundamentals from Units 6 and 12, we created a song recommender and validated the results based on how well it recommends songs to a genre-specific playlist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c33a4",
   "metadata": {},
   "source": [
    "## The Dataset  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f16a3a",
   "metadata": {},
   "source": [
    "The dataset we used is from the [Spotify Million Playlist Dataset Challenge](https://research.atspotify.com/the-million-playlist-dataset-remastered/). The data comes from sampling random existing Spotify playlists made by real users for Spotify's AI research (much of which focuses on recommendation systems).  \n",
    "Each playlist is a dictionary that includes data. The data given about each track is as follows:\n",
    "* **track_name** - the name of the track\n",
    "* **track_uri** - the Spotify URI of the track\n",
    "* **album_name** - the name of the track's album\n",
    "* **album_uri** - the Spotify URI of the album\n",
    "* **artist_name** - the name of the track's primary artist\n",
    "* **artist_uri** - the Spotify URI of track's primary artist\n",
    "* **duration_ms** - the duration of the track in milliseconds\n",
    "* **pos** - the position of the track in the playlist (zero-based)  \n",
    "\n",
    "Unlike other datasets we explored, the dataset does not include song attributes that can be compared to one another, things like tempo, genre, or danceability (although attributes are available using Spotify Web API). Since our model is trained on the occurence of songs in a certain type of playlist, the data is only used to identify unique songs in the dataset.  \n",
    "\n",
    "To use the dataset, it was unzipped then made into a datafram using this `for loop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c330b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f04927",
   "metadata": {},
   "source": [
    "To make the data easier to handle, we shortened playlists to 30 songs and deleted playlists with less than 30 songs. This ensures that every playlist has the same size and will work with tensors.  \n",
    "\n",
    "Next, we create the 10 training sets by pulling out the last ten songs in each playlist, respectively assigning them as the target song for each of the training sets (train_1 uses the song at index 21, train_2 uses the song at index 22, etc) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78e76ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for sorting/cutting playlists\n",
    "# code for yeeting 10 songs into training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40946b4",
   "metadata": {},
   "source": [
    "The model needs to have each unique track labelled with a unique index in the set tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "612499b6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# for loops for song indeces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492add3f",
   "metadata": {},
   "source": [
    "Now each song can be identified with a unique id. We then go back through the playlists and replace the songs with ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bc7835d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# code for playlists, all playlists are just id numbers\n",
    "# make training playlists and playlist_list into tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e831623c",
   "metadata": {},
   "source": [
    "Let's look at the shapes of our tensors:\n",
    "\n",
    "Training shape is the number of playlists.    \n",
    "Playlists shape is the number for every playlist, there are 20 songs tensor[4..., 20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "610f590a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# tensor shape code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d4478",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5d7e45",
   "metadata": {},
   "source": [
    "On to the model. To set up the embedding of each song, we used the number of unique songs and the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77184f4a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# unique songs number\n",
    "# embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "595337a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# num parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c43f46f",
   "metadata": {},
   "source": [
    "Next, we inserted tensor of playlists into `embedder` to create a tensor that has the embeddings for each song in a given playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2c985b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for playlist embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95c8a03c",
   "metadata": {},
   "source": [
    "Our model is as follows:  \n",
    "<div>\n",
    "    <img src=\"model.png\" width=\"500\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c36b9",
   "metadata": {},
   "source": [
    "Let's talk about the MLPs. Each corresponds to the new parameter added:\n",
    "* Mean: the input is the embedding dimension, out put is also the embedding dimension. Mean is a tensor of length 50 and is the average of the rows for each embedding for the songs (an average of all the songs). \n",
    "* Variance: finds the difference ebtween all the songs. The final thing it inputs is the actual song to be recommended. The model then has two inputs.  \n",
    "* Head: has an extra layer to output the number of unique songs. We keep the input to 50 because the kernel keeps dying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1929e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for mean, variance, head "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170cb9a",
   "metadata": {},
   "source": [
    "Code for model setup based on the target with a single song embedding. Returns the output of the last mlp which is the large array that is softmaxed for probablities. Insert pretty picture here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a3d0367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64eee5e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce929b",
   "metadata": {},
   "source": [
    "Training the model: It goes through every playlist in the list \"playlist\" (it enumerates), taking the playlist and runs through the model, outputting a tensor with the probability of reommendation of each unique song. (big-ass array)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6d4b40",
   "metadata": {},
   "source": [
    "Cross-entropy takes two argument: the big-ass array of probailities of each unique song and the index of the target song."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3efd3f",
   "metadata": {},
   "source": [
    "Goes backward down the gradient to update each parameter in the model from the gradient. \n",
    "\n",
    "IMPORTANT: Zero the gradient every time the loss is used to back propogate to create the gradients. This saves memory space and keeps the kernel alive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10f9d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9696a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for setting up training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10416ed9",
   "metadata": {},
   "source": [
    "## Optimizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc52c5e",
   "metadata": {},
   "source": [
    "The optimization of the model comes from adjusting the following 4 parameters:\n",
    "* **learning rate**: how fast the weights change as it goes backwards along the gradient in the mlp  \n",
    "* **files**: changes how many playlists are loaded in  \n",
    "* **training set**: how many traning sets the model runs through  \n",
    "* **rounds**: how many times the whole process is repeated  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9adea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for changing parameters maybe here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73afa65",
   "metadata": {},
   "source": [
    "How we do epochs/rounds: For loop that runs every playlist through the model and back propogates the loss. For each time is runs through the epoch, there is alaso a loop for it to run throguh the number of training sets it wants. Nested in the loops is the call to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1149de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop for epochs/rounds \n",
    "# outputs training set number based on epoch round"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d4f84",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ec3501",
   "metadata": {},
   "source": [
    "show_recommendations: shows the top 15 songs that the model recommends. It takes in the index of the plalist (index 25 is a country playlist). It shows what each training set had as it's training target and then the actual song it recommended with the confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67b3445a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# code for show_recommendations with top 15 songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dbe221",
   "metadata": {},
   "source": [
    "To validate the model, we analyzed the number of country songs were recommended (generally most distinguishable genre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c1b44db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for get_playlist(25) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205528c1",
   "metadata": {},
   "source": [
    "The validation is to show that the model can at least recognize genres of songs and recommend a song within that genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fea83fcb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# table for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2dca9",
   "metadata": {},
   "source": [
    "playlist(7): rap/hip-hop\n",
    "playlist(25) and (31): country\n",
    "playlist(83): Christmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1530fe",
   "metadata": {},
   "source": [
    "## Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e630d2d7",
   "metadata": {},
   "source": [
    "Upping training set, modularizing model to just change parameters, going from concatenating embeddings to using a sequential model\n",
    "\n",
    "In terms of optimizing parameters, blah blah blah smart smart\n",
    "\n",
    "Removing one of the mlps since target is not added into the training portion. Results will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f7ea3",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334160a1",
   "metadata": {},
   "source": [
    "Cross-entropy loss function\n",
    "Loss.backward() carries the loss through the whole model\n",
    "\n",
    "Counting country songs in output? Does this count?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbb2797",
   "metadata": {},
   "source": [
    "## Alternative Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf72fb",
   "metadata": {},
   "source": [
    "Many different ways to set up this sequential model  \n",
    "\n",
    "Earlier idea of concatenating mean, variance, and target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b72be1c",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10d2d5f",
   "metadata": {},
   "source": [
    "Obviously, our model does not perform the way we intended it to. Like all model training, the best possible song recommender needs lots of data and subsequent time to train on (some recommenders we saw online took 3-4 days to train)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
