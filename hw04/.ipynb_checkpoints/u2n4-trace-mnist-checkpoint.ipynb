{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96848edf",
   "metadata": {},
   "source": [
    "# Trace Simple Image Classifier\n",
    "\n",
    "Task: trace and explain the dimensionality of each tensor in a simple image classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e7193",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf51f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717f1e8",
   "metadata": {},
   "source": [
    "Get some example digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9095b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6cd3f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6131, 6265)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threes = (path/'train'/'3').ls().sorted()\n",
    "sevens = (path/'train'/'7').ls().sorted()\n",
    "len(threes), len(sevens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8180c642",
   "metadata": {},
   "source": [
    "Here is one image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54fa7557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA9ElEQVR4nM3Or0sDcRjH8c/pgrfBVBjCgibThiKIyTWbWF1bORhGwxARxH/AbtW0JoIGwzXRYhJhtuFY2q1ocLgbe3sGReTuuWbwkx6+r+/zQ/pncX6q+YOldSe6nG3dn8U/rTQ70L8FCGJUewvxl7NTmezNb8xIkvKugr1HSeMP6SrWOVkoTEuSyh0Gm2n3hQyObMnXnxkempRrvgD+gokzwxFAr7U7YXHZ8x4A/Dl7rbu6D2yl3etcw/F3nZgfRVI7rXM7hMUUqzzBec427x26rkmlkzEEa4nnRqnSOH2F0UUx0ePzlbuqMXAHgN6GY9if5xP8dmtHFfwjuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F1AAC39EC70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_3 = Image.open(threes[1])\n",
    "example_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d16aed",
   "metadata": {},
   "source": [
    "To prepare to use it as input to a neural net, we first convert integers from 0 to 255 into floating point numbers between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7406d76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_3_tensor = tensor(example_3).float() / 255\n",
    "example_3_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de44e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = example_3_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e66e8",
   "metadata": {},
   "source": [
    "Our particular network will ignore the spatial relationship between the features; later we'll learn about network architectures that do pay attention to spatial neighbors. So we'll *flatten* the image tensor into 28\\*28 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c43e173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_3_flat = example_3_tensor.view(width * height)\n",
    "example_3_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee9e07",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fcae10",
   "metadata": {},
   "source": [
    "We'll define a simple neural network (in the book, a 3-vs-7 classifier) as the sequential combination of 3 layers. First we define each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca205bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layers. This is where you'll try changing constants.\n",
    "linear_1 = nn.Linear(in_features=784, out_features=30)\n",
    "relu_layer = nn.ReLU()\n",
    "linear_2 = nn.Linear(in_features=30, out_features=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90861399",
   "metadata": {},
   "source": [
    "Then we put them together in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4502d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net = nn.Sequential(\n",
    "    linear_1,\n",
    "    relu_layer,\n",
    "    linear_2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b2970",
   "metadata": {},
   "source": [
    "Each of `nn.Linear`, `nn.ReLU`, and `nn.Squential` are PyTorch *modules*. We can *call* a module with some input data to get the output data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b55251f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1666, -0.1924], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_net(example_3_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1189506e",
   "metadata": {},
   "source": [
    "Your turn:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479d8e7",
   "metadata": {},
   "source": [
    "1. Obtain the same result as the line above by applying each layer in sequence.\n",
    "\n",
    "The outputs of each layer are called *activations*, so we can name the variables `act1` for the activations of layer 1, and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ffb847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = example_3_flat\n",
    "act1 = linear_1(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8f52ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "act2 = relu_layer(act1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06cc6aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "act3 = linear_2(act2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555cf6ec",
   "metadata": {},
   "source": [
    "2. Evaluate `act1`, `act2`, and `act3`. (Code already provided; look at the results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71fb50cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1971, -0.2886,  0.2023, -0.0984,  0.1338, -0.1604,  0.2701, -0.3103,  0.2313,  0.1280, -0.3245,  0.1302, -0.1761, -0.1394,  0.0234, -0.1384,  0.3531,  0.5236, -0.1388,  0.1109,  0.0033,\n",
       "         0.1793, -0.3673, -0.0706, -0.1324, -0.4853,  0.3566,  0.1476, -0.2868, -0.0929], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "717a140f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.2023, 0.0000, 0.1338, 0.0000, 0.2701, 0.0000, 0.2313, 0.1280, 0.0000, 0.1302, 0.0000, 0.0000, 0.0234, 0.0000, 0.3531, 0.5236, 0.0000, 0.1109, 0.0033, 0.1793, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.3566, 0.1476, 0.0000, 0.0000], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "021ede80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1666, -0.1924], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff68adbd",
   "metadata": {},
   "source": [
    "2. Evaluate the `shape` of `act1`, `act2`, and `act3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e8cf98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7ebf842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "729f5165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9271b1",
   "metadata": {},
   "source": [
    "3. Write expressions for the shapes of each activation in terms of `linear_1.in_features`, `linear_2.out_features`, etc. (ignore the `torch.Size(` part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffd3a3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act1_shape = linear_1.out_features\n",
    "act1_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6b3dc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act2_shape = linear_2.in_features\n",
    "act2_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e160a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act3_shape = linear_2.out_features\n",
    "act3_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ed4000",
   "metadata": {},
   "source": [
    "4. Evaluate the `shape` of `linear_1.weight`, `linear_1.bias`, and the same for `linear_2`. Write expressions that give the value of each shape in terms of the `in_features` and other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85b69231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 784])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4a64313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_1.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d809043f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_2.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de3bc7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_2.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62b2e942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 784]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_1_weight_shape = [linear_1.out_features, linear_1.in_features] \n",
    "linear_1_weight_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24bcfa7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_1_bias_shape = linear_1.out_features\n",
    "linear_1_bias_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be4c6c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 30]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_2_weight_shape = [linear_2.out_features, linear_2.in_features]\n",
    "linear_2_weight_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce5b9875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_2_bias_shape = linear_2.out_features\n",
    "linear_2_bias_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76d8e1",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb578c72",
   "metadata": {},
   "source": [
    "1. Try changing each of the constants provided to the `nn.Linear` modules. Identify an example of:\n",
    "  1. A constant that can be freely changed in the neural net definition. \n",
    "  2. A constant that cannot be changed because it depends on the input. \n",
    "  3. A pair of constants that must be changed together. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292a8c7c",
   "metadata": {},
   "source": [
    "   A. The out_feature of linear_2.  \n",
    "   B. The in_feature of linear_1.  \n",
    "   C. The out_feature of linear_1 and the in_feature of linear_2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622d971",
   "metadata": {},
   "source": [
    "2. Describe the relationship between the values in  `act1` and `act2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f5d8f1",
   "metadata": {},
   "source": [
    "act2 is like act1 except it changes the negative values in act1 to zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
