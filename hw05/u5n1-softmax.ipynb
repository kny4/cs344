{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax, part 1\n",
    "\n",
    "Task: practice using the `softmax` function.\n",
    "\n",
    "**Why**: The softmax is a building block that is used throughout machine learning, statistics, data modeling, and even statistical physics. This activity is designed to get comfortable with how it works at a high and low level.\n",
    "\n",
    "**Note**: Although \"softmax\" is the conventional name in machine learning, you may also see it called \"soft *arg* max\". The [Wikipedia article](https://en.wikipedia.org/w/index.php?title=Softmax_function&oldid=1065998663) has a good explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function defines `softmax` by using PyTorch built-in functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_torch(x):\n",
    "    return torch.softmax(x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on an example tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tensor([1., 2., 3.])\n",
    "softmax_torch(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start by playing with the interactive widget below. Describe the outputs when:\n",
    "\n",
    "    1. All of the inputs are the same.\n",
    "    2. One input is much bigger than the others.\n",
    "    3. One input is much smaller than the others.\n",
    "\n",
    "Finally, describe the input that gives the largest possible value for output 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e045c3806064a8bb41eedb3d18a4d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='x0', max=2.0, min=-2.0), FloatSlider(value=0.0, descâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = 2.0\n",
    "@widgets.interact(x0=(-r, r), x1=(-r, r), x2=(-r, r))\n",
    "def show_softmax(x0, x1, x2):\n",
    "    x = tensor([x0, x1, x2])\n",
    "    xs = softmax_torch(x)\n",
    "    plt.barh([2, 1, 0], xs)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.yticks([2, 1, 0], ['output 0', 'output 1', 'output 2'])\n",
    "    plt.ylabel(\"softmax(x)\")\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. When all the outputs are the same, the outputs are evenly divided from a total value of 1, so the tensor values are [0.3333, 0.3333, 0.3333].  \n",
    "B. When one output is much larger than the others, the outputs are weighted so that the input that is the largest has the largest value in the tensor and the other two values have the same value as each other, splitting the difference between the two with what is left over from the largest output. The tensor values when x0 = 2 and x1 = x2 = -2 are [0.9647, 0.0177, 0.0177] which sum to approximately 1.  \n",
    "C. When one output is much smaller than the other two, the three numbers still sum to 1, with the smallest output being close to zero and the other two large outputs evenly split with the rest. The tensor values when x2 = -2 and x0 = x1 = 2 are [0.4955, 0.4955, 0.0091].  \n",
    "D. The largest value for output 1 (or any of the outputs) occurs when the input 1 is at its largest value and x0 and x2 are at their smallest values. Since all three outputs are split and weighted to equal one, this makes sense! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fill in the following function to implement softmax yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(xx):\n",
    "    # Exponentiate x so all numbers are positive.\n",
    "    expos = xx.exp()\n",
    "    assert expos.min() >= 0\n",
    "    # Normalize (divide by the sum).\n",
    "    return expos / expos.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluate `softmax(x)` and verify that it is close to the `softmax_torch(x)` you evaluated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Evaluate `softmax_torch(__)` for each of the following expressions. Observe how each output relates to `softmax_torch(x)`.\n",
    "\n",
    "- `x + 1`\n",
    "- `x - 100`\n",
    "- `x - x.max()`\n",
    "- `x * 0.5`\n",
    "- `x * 3.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0024, 0.0473, 0.9503])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_torch(x * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x + 1 = tensor([0.0900, 0.2447, 0.6652]) --> same as soft_max(x)    \n",
    "x - 100 = tensor([0.0900, 0.2447, 0.6652]) --> same as softmax(x)    \n",
    "x - x.max() = tensor([0.0900, 0.2447, 0.6652]) --> same as softmax(x)  \n",
    "x * 0.5 = tensor([0.1863, 0.3072, 0.5065]) --> tensor values / 0.5    \n",
    "x * 3.0 = tensor([0.0024, 0.0473, 0.9503]) --> tensor values / 3   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. *Numerical issues*. Assign `x2 = 50 * x`. Try `softmax(x2)` and observe that the result includes the dreaded `nan` -- \"not a number\". Something went wrong. **Evaluate the first mathematical operation in `softmax`** for this particularly problematic input. You should see another kind of abnormal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., nan, nan])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = 50 * x\n",
    "softmax(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., nan, nan])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(50 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. *Fixing numerical issues*. Now try `softmax(x2 - 150.0)`. Observe that you now get valid numbers. Also observe how the constant we subtracted relates to the value of `x2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7835e-44, 1.9287e-22, 1.0000e+00])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x2 - 150.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 50., 100., 150.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Copy your `softmax` implementation to a new function, `softmax_stable`, and change it so that it subtracts `xx.max()` before exponentiating. (Don't use any in-place operations.) Verify that `softmax_stable(x2)` now works, and obtains the same result as `softmax_torch(x2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_stable(xx):\n",
    "    # Subtract max and then exponentiate x so all numbers are positive.\n",
    "    expos = (xx - xx.max()).exp()\n",
    "    assert expos.min() >= 0\n",
    "    # Normalize (divide by the sum).\n",
    "    return expos / expos.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7835e-44, 1.9287e-22, 1.0000e+00])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_torch(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7835e-44, 1.9287e-22, 1.0000e+00])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_stable(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following situation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., -1.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = tensor([1., 0.,])\n",
    "x3 = x2 - 1\n",
    "x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 0.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x4 = x2 * 2\n",
    "x4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Are `softmax(x2)` and `softmax(x3)` the same or different? How could you tell without having to evaluate them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax(x2) and softmax(x3) are the same because the change in input is subtraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Are `softmax(x2)` and `softmax(x4)` the same or different? How could you tell without having to evaluate them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax(x2) and softmax(x4) are different because the change in input is multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explain why `softmax(x2)` failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax(x2) failed because the tensor values are divided by a large enough number that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use your observations in \\#1-2 above to explain why `softmax_stable` still gives the correct answer even though we changed the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax_stable still gives the correct answer because subtracting the maximum value ensures that the sum of the tensor values will be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Explain why `softmax_stable` doesn't give us infinity or Not A Number anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax_stable doesn't output inf or nan because"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension *optional*\n",
    "\n",
    "Try to prove your observation in Analysis \\#1 by symbolically simplifying the expression `softmax(logits + c)` and seeing if you can get `softmax(logits)`. Remember that `softmax(x) = exp(x) / exp(x).sum()` and `exp(a + b) = exp(a)exp(b)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
