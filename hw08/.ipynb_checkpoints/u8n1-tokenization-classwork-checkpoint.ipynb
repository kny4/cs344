{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jc8Qlh1TEgC"
   },
   "source": [
    "# Tokenization\n",
    "\n",
    "Task: Convert text to numbers; interpret subword tokenization.\n",
    "\n",
    "There are various different ways of converting text to numbers. This assignment works with one popular approach: assign numbers to parts of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8_8RWp3TX-8"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUvTIxyWTdBF"
   },
   "source": [
    "We'll be using the HuggingFace Transformers library, which provides a (mostly) consistent interface to many different language models. We'll focus on the OpenAI GPT-2 model, famous for OpenAI's assertion that it was \"too dangerous\" to release in full.\n",
    "\n",
    "[Documentation](https://huggingface.co/transformers/model_doc/gpt2.html) for the model and tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you're running this on the lab machines, you should **re-run the class setup script**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anaconda is set up.\r\n",
      "TORCH_HOME looks ok.\r\n",
      "HF_HOME looks ok.\r\n",
      "Scratch configured in ~/.fastai/config.ini.\r\n",
      "Done.\r\n"
     ]
    }
   ],
   "source": [
    "!/home/cs/344/setup-cs344.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you will need to **LOG OUT AND LOG BACK IN**. (If you know what you're doing and want to avoid the log out: that added a definition of `HF_HOME` to `~/.profile`; you can set it here with `os.environ` if you want.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's install the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWy--2nwhWPy",
    "outputId": "44b8e674-7e8b-4cf6-a1e9-1f8d62740382"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "osKgPaDwhaN4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiNKbIh8hyDg"
   },
   "source": [
    "### Download and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IM5o_4w1hfyV"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", add_prefix_space=True) # smaller version of GPT-2\n",
    "# Alternative to add_prefix_space is to use `is_split_into_words=True`\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-Z9_U0LUEVQ",
    "outputId": "1d639faf-5b56-4bb2-81e5-054ee086ef0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenizer has 50257 strings in its vocabulary.\n",
      "The model has 81,912,576 parameters.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The tokenizer has {len(tokenizer.get_vocab())} strings in its vocabulary.\")\n",
    "print(f\"The model has {model.num_parameters():,d} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phrase = \"May the Force be\" [damned]\n",
    "phrase = \"To be or not to\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[1675,  307,  393,  407,  284]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = tokenizer(phrase, return_tensors='pt')\n",
    "# Batch pf phrases that can passed through model at the same time, helps for fast training\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape\n",
    "# [batch size, number of input ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ĠTo']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([1675])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ĠTo', 'Ġbe', 'Ġor', 'Ġnot', 'Ġto']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(batch['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.forward(**batch) #,labels=masked_input_ids, output_hidden_states=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = batch ['input_ids']\n",
    "transformer_outputs = model.transformer(input_ids)\n",
    "hidden_states = transformer_outputs[0]\n",
    "lm_logits = model.lm_head(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=50257, bias=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0535e-01, -3.1119e-01,  1.8547e-01, -2.1496e-01,  4.7452e-02,\n",
       "         1.0167e-01, -5.4309e-01,  1.0478e+00,  1.3330e-01,  3.2923e-01,\n",
       "         3.2632e-01,  1.2584e-01, -1.9127e-01,  4.0045e-01, -2.8676e-02,\n",
       "        -2.3159e-01,  3.8155e-02, -8.4357e-01, -1.5539e-01,  1.4244e-01,\n",
       "        -1.4554e-01, -5.5682e-02,  1.4789e-01,  4.7947e-01,  2.0871e-01,\n",
       "         8.7035e-03,  3.0157e-01, -3.0594e-01,  2.3922e-01,  3.7042e-01,\n",
       "         1.3656e-01,  2.4162e-02,  1.9577e-01, -1.6666e-01,  2.0682e-01,\n",
       "        -4.3732e-01,  3.8166e+01,  3.9355e-01, -1.4582e-01,  2.6889e-01,\n",
       "        -1.8951e-01, -5.9843e-02, -8.7268e-03,  1.4158e-01,  1.8710e-02,\n",
       "         2.4323e-01,  1.1892e-01, -2.4620e-01,  2.2336e-01,  5.8412e-01,\n",
       "        -1.0754e-01,  1.5730e-01, -2.6348e-01, -1.1958e-01,  7.4822e-02,\n",
       "         1.4612e+00, -2.7473e-02, -5.6583e-01, -2.1155e-01,  1.7774e-01,\n",
       "         8.1083e-02, -2.7762e-01,  2.4878e-01,  3.9668e-02, -1.2293e+00,\n",
       "        -1.6452e-02,  1.7181e-01,  7.7464e-02, -1.3782e+00, -4.7965e-03,\n",
       "         2.0650e-03,  2.8206e-01,  3.5152e-01, -4.9678e-01,  1.5206e-01,\n",
       "        -4.1769e-01,  1.1165e-01, -6.8212e-02, -5.2039e-02,  1.9944e-01,\n",
       "        -3.6044e-01, -1.4607e-01,  1.5313e-01,  1.3124e-01, -1.1925e-01,\n",
       "         3.2101e-01, -4.7111e-01, -1.1342e+00, -3.4032e-02, -1.7248e-01,\n",
       "        -1.1817e-01, -4.5333e-01,  6.1987e-03,  2.6574e-01,  3.3121e-01,\n",
       "        -1.4281e-01,  3.1524e-01, -3.2376e-01, -1.1187e-01,  2.3965e-01,\n",
       "         1.1164e-01,  8.9450e-01,  3.0482e+00, -9.3317e-02,  3.6057e-01,\n",
       "         3.5810e-01,  6.5613e-02, -7.9306e-01,  2.0276e-01, -5.0280e-01,\n",
       "         3.3830e-01, -9.6329e-02, -5.3231e-01, -1.3656e-01, -1.3127e-01,\n",
       "         5.4469e-02,  1.3128e-01,  3.4107e-01, -2.9798e-01,  6.4453e-01,\n",
       "        -1.7681e-01, -1.6725e-01,  1.6332e-01, -7.7117e-02, -3.5893e-01,\n",
       "         2.3537e-02,  2.6719e-01, -9.3135e-02, -3.7750e-01, -5.4930e-02,\n",
       "        -2.7499e-01, -5.5832e-01, -7.5405e-01,  8.3843e-02,  1.9469e-01,\n",
       "         8.6336e-02, -2.6438e-01,  2.1632e-01,  5.4121e-01,  7.0691e-03,\n",
       "        -3.5829e-02,  3.5688e-01,  5.4836e-01,  2.3452e-01,  5.8880e-01,\n",
       "        -2.1701e-01,  1.0695e-01, -2.2124e-01,  1.8760e-01, -9.6945e-01,\n",
       "        -2.5964e-01,  2.7497e-01,  3.5022e-01, -5.9898e-01, -1.1751e-01,\n",
       "         2.7289e-01, -4.0439e-01,  9.3946e-02,  3.6774e-01,  7.1675e-02,\n",
       "        -4.7276e-02, -2.2279e-01,  1.7840e-02, -3.8499e-02,  1.1272e-01,\n",
       "         2.4724e-01, -3.3316e-01,  1.4154e+00,  1.2875e-01,  2.6582e-01,\n",
       "        -3.6213e-01,  2.1599e-01, -5.8211e-01,  1.5934e-01,  1.0338e-03,\n",
       "         2.9766e-01,  4.4888e-01,  1.8597e-01, -8.7129e-02,  3.4873e-01,\n",
       "         1.3821e-02,  6.2897e-02,  8.0556e-02,  5.2856e-02,  1.6276e-01,\n",
       "        -8.9821e-02, -2.0436e-01,  5.4932e-01,  2.6791e-01,  4.2092e-01,\n",
       "         1.5491e-01, -1.2805e-01, -4.6686e-02, -2.8148e-01, -4.6668e-02,\n",
       "         3.2195e-01, -4.5638e-01,  4.4658e-01,  1.8976e-02, -6.1712e-01,\n",
       "         3.0964e-01,  5.2028e-01, -1.2331e-01, -3.3363e-01, -3.1771e-02,\n",
       "        -8.3861e-02,  7.6691e-02, -2.3511e-01,  8.8402e-02, -5.4336e-02,\n",
       "        -2.9757e-02,  4.3524e-01, -1.6115e-01, -4.6398e-01, -4.1834e-01,\n",
       "         4.4826e-01, -5.7352e-02,  4.2320e-01, -1.0007e-01,  8.5917e-03,\n",
       "        -2.0489e-02,  1.7302e-01,  1.9966e-01, -3.5228e-02,  8.1705e-01,\n",
       "        -2.6878e-01,  5.3105e-01, -4.8178e-01,  4.5102e-01,  1.1085e-01,\n",
       "         2.5945e-01,  6.1352e-02, -8.3558e-02, -3.2812e-01,  2.8061e-01,\n",
       "         1.4206e-01, -2.4469e-01, -2.3985e-03, -6.3692e-01,  3.1452e-01,\n",
       "         1.6638e-01, -1.4503e-01, -1.2795e-01,  2.3044e-01,  1.1763e-01,\n",
       "        -6.0785e-02,  3.1269e-01, -9.7290e-01, -2.8065e-01,  1.4846e-02,\n",
       "         2.8642e-01, -9.7486e-01,  4.9641e-02,  5.6704e-01, -1.4050e-01,\n",
       "         1.7517e-01,  6.6442e-01, -2.8611e-02,  1.9718e-01,  7.0208e-01,\n",
       "        -2.5780e-01, -5.9233e-01,  7.6205e-02,  8.8734e-02, -3.6524e-01,\n",
       "         1.0331e-01, -1.1111e+00, -2.4831e-01, -4.1087e-01, -2.5125e-01,\n",
       "        -2.0790e-01, -1.7778e+00, -3.4871e-01,  5.5980e-01, -5.7069e-01,\n",
       "        -9.6738e-02, -1.3599e-01,  2.2751e-01,  5.9951e-03, -8.4967e-02,\n",
       "         1.2733e-03,  4.2194e-02,  3.4745e-01, -5.3361e-02,  1.8156e-01,\n",
       "        -1.6574e-01,  4.2702e-02, -1.5977e-01, -1.0084e-01, -4.3522e-01,\n",
       "        -5.0114e-02,  4.9770e-01, -1.3269e-01,  2.9031e-01, -4.4443e-01,\n",
       "        -2.0096e-01, -1.2213e-01,  6.3616e-01,  2.4200e-01,  2.3859e-01,\n",
       "         1.9711e-01,  6.9031e-02, -3.4826e-01, -5.1577e-02, -1.6281e-01,\n",
       "         3.2786e-01,  3.2962e-01, -2.2668e-01, -1.5182e+00,  3.9426e-02,\n",
       "         3.2075e-01, -5.8809e-01,  7.0172e-03, -2.6433e-01, -8.2938e+00,\n",
       "        -4.0696e-02, -3.6885e-02,  3.3908e-01,  1.6092e-01,  5.0510e-01,\n",
       "         1.0303e+00,  3.4946e-01,  1.3238e-01,  6.6164e-02,  3.8504e-01,\n",
       "         2.2406e-01,  4.3966e-02, -2.1302e-01, -5.2690e-01, -4.6567e-01,\n",
       "        -2.7645e-01, -1.1757e-01,  4.7593e-01,  2.3584e-02, -2.1249e-01,\n",
       "         5.9190e-02, -5.7396e-01, -7.2010e-02,  9.8359e-02,  2.1312e-01,\n",
       "         2.3556e-02,  3.1787e-01,  2.8435e-01, -3.2684e-01,  1.6078e-01,\n",
       "         6.6896e-02, -6.2252e-02, -2.7597e-01,  4.7845e-02,  1.1386e-02,\n",
       "        -3.3925e-01, -1.0156e+00, -2.3687e-01,  7.8124e-01,  6.1640e-02,\n",
       "        -3.2387e-01, -1.8392e-01, -3.0674e-01,  4.6902e-02, -7.3829e-03,\n",
       "         2.1247e-01,  2.0752e-02, -3.0227e-02,  4.6875e-02, -4.4891e-01,\n",
       "        -5.9562e-01,  1.6582e-01,  1.4771e-01, -1.3095e-01,  1.8857e-01,\n",
       "         3.7394e-01, -5.6598e-02,  3.9595e-01, -1.2493e+00, -1.0839e+01,\n",
       "         6.7440e-01,  3.1498e-01,  1.5909e+00, -2.0962e-01, -1.5351e-01,\n",
       "         1.4968e-01, -2.0375e-01, -1.6402e-01,  3.4345e-01, -2.2986e-01,\n",
       "        -3.1725e-04,  9.9978e-02,  4.8912e-01, -3.9166e-02,  3.1650e-01,\n",
       "        -4.0692e-02, -4.1817e-02,  2.7997e-01,  4.5549e-01,  6.7874e-01,\n",
       "         1.8205e-01,  4.8802e-01,  3.2423e-01, -2.7739e-01,  1.1796e-01,\n",
       "         3.5844e-01, -2.4654e-01,  1.6719e-03, -7.1080e-02,  2.6402e-01,\n",
       "        -1.1098e-03,  1.4772e-02, -2.8923e-01, -1.1514e+00, -3.9161e-01,\n",
       "        -3.8060e-01,  2.6208e-01, -9.4407e-02,  1.8417e-01, -1.4444e-01,\n",
       "         1.2065e-02,  3.1023e-01, -1.8010e-01,  6.7943e-01,  1.3402e-01,\n",
       "         1.6368e-01,  3.8118e-01,  3.4880e-01,  1.0670e-01, -5.6217e-02,\n",
       "        -3.1597e-01, -1.9621e-01, -8.1803e-02, -1.9050e-01,  3.2979e-01,\n",
       "         5.5018e+01,  1.4680e-02, -5.6221e-02, -3.7701e-01,  2.9680e-01,\n",
       "         2.7636e-01, -1.3665e-01, -2.4362e-01, -9.9431e-02, -1.4564e-01,\n",
       "        -3.6089e-01,  3.6707e-01,  4.5464e-03, -9.0150e-02,  1.6648e-02,\n",
       "         5.8711e-01, -2.0519e-01,  1.1178e+00, -3.8200e-01,  3.6346e-01,\n",
       "        -4.9197e-01, -1.3691e-01,  4.9107e-02,  1.1149e-02, -1.4377e-01,\n",
       "         1.1335e+00, -6.1664e-01,  1.5240e-01, -5.0025e-01, -1.8882e-01,\n",
       "        -1.1820e-01,  2.6372e-01, -1.1726e-02, -4.2407e-01,  4.9973e-01,\n",
       "         2.4944e-01,  1.1977e-02,  5.8535e-02,  4.2565e-02, -8.7332e-02,\n",
       "         2.4631e-01,  3.1869e-01, -5.5482e-01,  2.6366e-01,  4.7947e-01,\n",
       "        -2.2420e-01, -2.5869e-01, -3.7366e-01, -2.8480e-01, -2.0413e+00,\n",
       "        -9.7992e-01,  1.2777e+00, -3.3962e-02, -2.4908e-01,  3.9765e-01,\n",
       "         2.7099e-01,  6.6650e-02, -1.8013e-01,  1.5997e-01,  2.1526e-01,\n",
       "        -1.5618e-01,  2.6571e-01,  6.9165e-02, -7.1499e-02, -5.9999e-01,\n",
       "         5.2817e-02,  1.3601e+02,  7.6149e-01,  8.5568e-02,  9.8527e-02,\n",
       "         3.6085e-01,  2.9218e-01, -2.5562e-01,  9.2839e-01,  5.8299e-01,\n",
       "        -2.6899e-01,  5.5221e-03,  4.4073e-01,  2.0448e-02,  4.2147e-01,\n",
       "        -1.7833e-01, -6.7605e-01,  4.4854e-01, -3.1857e-01, -1.1279e-01,\n",
       "         8.7610e-02, -2.5335e-01, -1.2332e-01,  7.0627e-01, -8.0377e-03,\n",
       "         1.2581e-01,  2.4673e-01, -1.0047e-01, -9.1683e-02, -8.3957e-02,\n",
       "        -3.4477e-01, -4.4668e-01,  6.7680e-01, -2.0350e-01,  3.7050e-01,\n",
       "         1.8212e-01,  2.9150e-01,  1.5823e-01,  2.0629e-01, -6.8184e-02,\n",
       "         1.3789e-02,  1.1985e-01, -6.6547e-02, -2.9255e-01, -5.2233e-01,\n",
       "         2.4582e-01,  1.6114e-01, -1.3174e-01,  2.2633e-01, -6.1084e-01,\n",
       "         3.2183e-01, -1.7555e-01, -1.5066e-01, -3.6774e-03, -2.0220e-01,\n",
       "        -1.0727e-01,  1.1613e-01, -2.3220e-01, -3.5989e-01, -1.0219e+00,\n",
       "        -3.4572e-01, -1.3210e-01,  3.0662e-01,  2.8911e-01, -4.0213e-01,\n",
       "         3.3075e-01, -9.3898e-02,  9.7447e-02, -5.3394e-02, -3.4667e-02,\n",
       "        -1.1461e-01,  1.4851e+00,  1.2516e-01,  2.5392e-01,  1.0495e-01,\n",
       "        -1.4957e-01,  3.1762e-01,  3.9925e-02, -6.2985e-02,  7.2826e-02,\n",
       "        -2.1803e-01,  1.2564e-01, -9.5189e-02,  6.2716e-02,  1.4356e-01,\n",
       "         3.8741e-01,  1.5603e-01, -7.8383e-02, -6.1202e-02,  4.6049e-02,\n",
       "         2.6784e-01,  1.3110e-01,  1.4270e-01,  5.2299e-02, -2.0181e-02,\n",
       "         1.1583e-01, -2.2044e-01,  5.4385e-01,  2.5105e-01,  2.0100e-02,\n",
       "        -1.3401e-02, -2.1138e-01,  9.9809e-02, -1.5349e-01,  3.3680e-01,\n",
       "        -2.7985e-01, -2.8506e-02,  2.1130e-01, -4.9890e-02,  9.7220e-03,\n",
       "        -9.7498e-02, -2.4768e-01, -2.5438e-01, -3.9689e-01, -3.2130e-01,\n",
       "         1.2081e-01,  7.0722e-02,  1.1873e-01,  8.0588e-02, -2.1653e-01,\n",
       "         2.6573e-01, -9.7069e-02,  9.7872e-03, -2.7814e-01, -8.8303e-02,\n",
       "         2.4810e-01,  1.6866e-01, -3.6802e-01, -2.2619e-01, -6.5739e-01,\n",
       "        -1.8239e-01, -2.5889e-01, -2.7157e-01,  1.1445e-01, -2.5604e-01,\n",
       "        -2.8511e-02, -4.4002e-01, -5.1067e-02,  1.6683e-01,  7.0000e-03,\n",
       "        -4.3807e-02,  9.9025e-02, -6.7664e-02,  2.9191e-01, -3.6328e-01,\n",
       "        -1.2822e-01,  3.4619e-01,  8.3307e-02,  3.9028e-01,  5.4174e-01,\n",
       "         2.5838e-01,  5.0128e-01,  2.7213e-01,  2.6246e-05, -1.9488e-01,\n",
       "         1.9915e-01, -7.4467e-02, -1.4499e-01, -9.1093e-01, -7.0953e-02,\n",
       "        -1.9798e-01,  3.5444e-01, -1.0536e-01,  3.6964e-01, -1.1470e-01,\n",
       "        -4.3747e-01, -2.9386e-01, -4.0169e-01, -6.5597e-03,  1.7849e-01,\n",
       "        -8.8198e-02, -1.9453e-01, -8.9270e-02, -3.1880e-02, -4.9515e-02,\n",
       "         1.6151e-01,  1.4595e-01,  1.0406e-01,  2.9902e-02, -1.2371e-02,\n",
       "         7.3522e-02, -7.8600e-02,  7.6439e-02, -1.0790e-01,  9.3645e-01,\n",
       "        -2.9331e-01, -2.2383e-01,  3.4084e-01, -2.2728e-01, -1.9271e-01,\n",
       "        -4.3711e-01, -4.7328e-01, -1.1313e-02, -1.4199e-01,  3.0918e-03,\n",
       "         9.0074e-02, -6.9656e-01, -1.8564e-01,  6.9134e-02, -5.2013e-01,\n",
       "         4.2976e-01,  1.1905e-01,  2.4855e-01, -2.8852e-01, -1.6562e-01,\n",
       "         1.3579e-01, -4.1080e-02,  4.9879e-02, -1.4003e+00, -3.7832e-01,\n",
       "        -2.5569e-01,  1.0025e+00, -5.5409e-01,  1.0335e-01, -1.1209e-01,\n",
       "        -4.4881e-01, -2.9960e-01, -4.3923e-01,  1.7244e-01, -5.8042e-01,\n",
       "         2.2243e-01,  1.9927e-01, -1.8212e-01,  5.0364e-01, -4.3244e-03,\n",
       "         5.7918e-01, -2.6124e+00,  5.7415e-01,  6.6395e-02,  4.3549e-01,\n",
       "        -9.4775e-02, -2.3341e-01,  5.3731e-01,  1.7858e-01,  1.6223e-02,\n",
       "        -1.7266e-01, -4.9017e-02,  3.6689e-02, -4.8486e-01,  8.7000e-02,\n",
       "         3.3592e-02,  3.0011e-01, -7.5272e-02, -8.1331e-02,  8.3488e-01,\n",
       "        -5.3692e-01, -9.5109e-02, -5.2162e-01, -1.1418e-01,  8.6475e-02,\n",
       "         2.0472e-01,  1.9582e-01, -1.4462e-01,  3.8579e-01,  7.6440e-02,\n",
       "         6.1359e-02,  4.9808e-01,  8.0456e-02, -3.2042e-01, -2.4868e-01,\n",
       "        -1.6105e-01, -5.1335e-01, -3.1853e-01,  2.1878e-01, -5.4689e-02,\n",
       "        -1.0148e-01,  3.3681e-01,  2.7356e+00, -9.8376e-02, -6.7793e-02,\n",
       "        -4.8619e-03, -1.6587e-01, -6.9798e-02], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight[257].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-51.5463, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[0, 1] @ model.lm_head.weight[257]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 50257])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'logits', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(out).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 50257])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.shape\n",
    "# [batch size, input ids, number of strings in vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_275204/1799561207.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# selecting one dimension out makes it go away\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# logits that say what the next token could be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# number represents how likely that word is to being the next word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnext_token_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "next_token_logits = out.logits[0, -1]\n",
    "# selecting one dimension out makes it go away\n",
    "# logits that say what the next token could be\n",
    "# number represents how likely that word is to being the next word\n",
    "next_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6485, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_probs = next_token_logits.softmax(dim=0)\n",
    "# softmax can be shifted with offset, shows probabiltiy of next token\n",
    "next_token_probs.max()\n",
    "# find the word that is by finding the index of where the probability-word is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(307)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_probs.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6485, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_probs[307]\n",
    "# find probability of token form argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġbe']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert ids to tokens\n",
    "tokenizer.convert_ids_to_tokens([307])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġbe', 'Ġhave', 'Ġthe', 'Ġdo', ',']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find more than one word with high probability\n",
    "tokenizer.convert_ids_to_tokens(\n",
    "    next_token_probs.topk(5).indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġa',\n",
       " 'Ġable',\n",
       " 'Ġthe',\n",
       " 'Ġsure',\n",
       " 'Ġin',\n",
       " 'Ġused',\n",
       " 'Ġan',\n",
       " 'Ġmore',\n",
       " 'Ġon',\n",
       " 'Ġhonest']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take off a layer\n",
    "tokenizer.convert_ids_to_tokens(\n",
    "    lm_logits[0, 1]\n",
    "    .softmax(dim=0)\n",
    "    .topk(10)\n",
    "    .indices\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOUiz_PsUZgS"
   },
   "source": [
    "## Task\n",
    "\n",
    "Consider the following phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JS7Z-DjoUiLK"
   },
   "outputs": [],
   "source": [
    "phrase = \"I visited Muskegon\"\n",
    "# Another one to try later. This was a famous early example of the GPT-2 model:\n",
    "# phrase = \"In a shocking finding, scientists discovered a herd of unicorns living in\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting familiar with tokens\n",
    "\n",
    "1: Use `tokenizer.tokenize` to convert the phrase into a list of tokens. (What do you think the `Ġ` means?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hyq-5XWSUx_8",
    "outputId": "22efb7a8-37c5-46f0-e230-c3b8e5ad6bdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ĠI', 'Ġvisited', 'ĠMus', 'ke', 'gon']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Use `tokenizer.convert_tokens_to_string` to convert the tokens back into a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I visited Muskegon'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: Use `tokenizer.encode` to convert the original phrase into token ids. (*Note: this is equivalent to `tokenize` followed by `convert_tokens_to_ids`*.) Call the result `input_ids`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GkaoLSFMiHzb",
    "outputId": "18c6391e-a9aa-4d4c-dace-d49f8bbcba7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[314, 8672, 2629, 365, 14520]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(phrase)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: Turn `input_ids` back into a readable string. Try this two ways: (1) using `convert_ids_to_tokens` and (2) using `tokenizer.decode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I visited Muskegon'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using convert_ids_to_to\n",
    "tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ncSRaBaZix8R",
    "outputId": "204670f9-d7c4-4856-c804-a038b77ccd1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I visited Muskegon'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying what you learned\n",
    "\n",
    "5: Use `model.generate(tensor([input_ids]))` to generate a completion of this phrase. (Note that we needed to add `[]`s to give a \"batch\" dimension to the input.) Call the result `output_ids`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PZm3eIjjKCJ",
    "outputId": "1c4b1a63-de00-44f9-eee7-85714012dbc6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  314,  8672,  2629,   365, 14520,    11,   290,   314,   373,  6655,\n",
       "           284,  1064,   326,   262,  1748,   550,   407,   587,  1498,   284]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids = model.generate(tensor([input_ids]))\n",
    "output_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6: Convert your `output_ids` into a readable form. (Note: it has an extra \"batch\" dimension, so you'll need to use `output_ids[0]`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "2kKJ8rvijVez",
    "outputId": "386df167-0e88-45f1-b1a0-01beab1f0bc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I visited Muskegon, and I was surprised to find that the city had not been able to'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `generate` uses a greedy decoding by default, but it's highly customizable. We'll play more with it in later exercises. For now, if you want more interesting results, try:\n",
    "\n",
    "- Turn on `do_sample=True`. Run it a few times to see what it gives.\n",
    "- Set `top_k=5`. Or 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  314,  8672,  2629,   365, 14520,    11,   810,   262,  1748,   286,\n",
       "          2629,   365, 14520,   318,  5140,    13,   198,   198,   198,   198]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids_5 = model.generate(tensor([input_ids]), do_sample=True, top_k=5)\n",
    "output_ids_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I visited Muskegon, where the city of Muskegon is located.\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output_ids_5[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  314,  8672,  2629,   365, 14520,   329,   262,   938,  1115,   812,\n",
       "            13,   314,  2911,   339,   857,   407,   466,   884,   281,  2801]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids_50 = model.generate(tensor([input_ids]), do_sample=True, top_k=50)\n",
    "output_ids_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I visited Muskegon for the last three years. I hope he does not do such an ill'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output_ids_50[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNS7mRS03a7VSFcbdUnYf/k",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "012-tokenization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
