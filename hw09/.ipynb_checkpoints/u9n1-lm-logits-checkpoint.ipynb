{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jc8Qlh1TEgC"
   },
   "source": [
    "# Logits in Causal Language Models\n",
    "\n",
    "Task: Ask a language model for the most likely next tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8_8RWp3TX-8"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start in the same way as the tokenization notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "osKgPaDwhaN4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One step in this notebook will ask you to write a function. The most common error when function-ifying notebook code is accidentally using a global variable instead of a value computed in the function. This is a quick and dirty little utility to check for that mistake. (For a more polished version, check out [`localscope`](https://localscope.readthedocs.io/en/latest/README.html).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_global_vars(func, allowed_globals):\n",
    "    import inspect\n",
    "    used_globals = set(inspect.getclosurevars(func).globals.keys())\n",
    "    disallowed_globals = used_globals - set(allowed_globals)\n",
    "    if len(disallowed_globals) > 0:\n",
    "        raise AssertionError(f\"The function {func.__name__} used unexpected global variables: {list(disallowed_globals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiNKbIh8hyDg"
   },
   "source": [
    "Download and load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IM5o_4w1hfyV"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", add_prefix_space=True) # smaller version of GPT-2\n",
    "# Alternative to add_prefix_space is to use `is_split_into_words=True`\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-Z9_U0LUEVQ",
    "outputId": "cb7d4eb7-bc54-4583-dd10-e0cb185494da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenizer has 50257 strings in its vocabulary.\n",
      "The model has 81,912,576 parameters.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The tokenizer has {len(tokenizer.get_vocab())} strings in its vocabulary.\")\n",
    "print(f\"The model has {model.num_parameters():,d} parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOUiz_PsUZgS"
   },
   "source": [
    "## Task\n",
    "\n",
    "In the tokenization notebook, we simply used the `generate` method to have the model generate some text. Now we'll do it ourselves.\n",
    "\n",
    "Consider the following phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JS7Z-DjoUiLK"
   },
   "outputs": [],
   "source": [
    "phrase = \"This weekend I plan to\"\n",
    "# Another one to try later. This was a famous early example of the GPT-2 model:\n",
    "# phrase = \"In a shocking finding, scientists discovered a herd of unicorns living in\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Call the `tokenizer` on the phrase to get a `batch` that includes `input_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QpyeKakrjfpt"
   },
   "outputs": [],
   "source": [
    "batch = tokenizer(phrase, return_tensors='pt')\n",
    "input_ids = batch['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Call the `model` on the `input_ids`. Examine the shape of the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZEy1QBTDotjU",
    "outputId": "70a3312b-1bc7-47b0-c1d6-4739c93f56c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: [1, 5, 50257] \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # This PyTorch we don't need it to compute gradients for us.\n",
    "    model_output = model(input_ids)\n",
    "print(f\"logits shape: {list(model_output.logits.shape)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: Pull out the logits corresponding to the *last* token in the input phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_275074/2149993866.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlast_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_token_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "last_token_logits = model_output.logits[1]\n",
    "assert len(last_token_logits.shape) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4: Identify the token id and corresponding string of the most likely next token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_275074/1673492282.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmost_likely_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Most likely next token: {most_likely_token_id}, which corresponds to {repr(tokenizer.decode(most_likely_token_id))}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "most_likely_token_id = out.logits[0, -1]\n",
    "print(f\"Most likely next token: {most_likely_token_id}, which corresponds to {repr(tokenizer.decode(most_likely_token_id))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5: Use the `topk` method to find the top-10 most likely choices for the next token.\n",
    "\n",
    "*Note*: This uses Pandas to make a nicely displayed table, and a *list comprehension* to decode the tokens. You don't *need* to understand how this all works, but I highly encourage thinking about what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_595f1_row0_col1 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_595f1_row1_col1 {\n",
       "  background-color: #4897c4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_595f1_row2_col1 {\n",
       "  background-color: #c9cee4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_595f1_row3_col1 {\n",
       "  background-color: #d2d2e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_595f1_row4_col1 {\n",
       "  background-color: #d9d8ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_595f1_row5_col1 {\n",
       "  background-color: #e0dded;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_595f1_row6_col1, #T_595f1_row7_col1 {\n",
       "  background-color: #ece7f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_595f1_row8_col1 {\n",
       "  background-color: #fbf3f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_595f1_row9_col1 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_595f1_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"col_heading level0 col0\" >tokens</th>\n",
       "      <th class=\"col_heading level0 col1\" >probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_595f1_row0_col0\" class=\"data row0 col0\" > go</td>\n",
       "      <td id=\"T_595f1_row0_col1\" class=\"data row0 col1\" >0.188313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_595f1_row1_col0\" class=\"data row1 col0\" > take</td>\n",
       "      <td id=\"T_595f1_row1_col1\" class=\"data row1 col1\" >0.138116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_595f1_row2_col0\" class=\"data row2 col0\" > spend</td>\n",
       "      <td id=\"T_595f1_row2_col1\" class=\"data row2 col1\" >0.099369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_595f1_row3_col0\" class=\"data row3 col0\" > make</td>\n",
       "      <td id=\"T_595f1_row3_col1\" class=\"data row3 col1\" >0.096061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_595f1_row4_col0\" class=\"data row4 col0\" > do</td>\n",
       "      <td id=\"T_595f1_row4_col1\" class=\"data row4 col1\" >0.091928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_595f1_row5_col0\" class=\"data row5 col0\" > be</td>\n",
       "      <td id=\"T_595f1_row5_col1\" class=\"data row5 col1\" >0.088007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_595f1_row6_col0\" class=\"data row6 col0\" > attend</td>\n",
       "      <td id=\"T_595f1_row6_col1\" class=\"data row6 col1\" >0.081478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_595f1_row7_col0\" class=\"data row7 col0\" > visit</td>\n",
       "      <td id=\"T_595f1_row7_col1\" class=\"data row7 col1\" >0.081293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_595f1_row8_col0\" class=\"data row8 col0\" > run</td>\n",
       "      <td id=\"T_595f1_row8_col1\" class=\"data row8 col1\" >0.069480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_595f1_row9_col0\" class=\"data row9 col0\" > have</td>\n",
       "      <td id=\"T_595f1_row9_col1\" class=\"data row9 col1\" >0.065956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x10c4af850>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_likely_tokens = last_token_logits.topk(...)\n",
    "most_likely_tokens_df = pd.DataFrame({\n",
    "    'tokens': [tokenizer.decode(...) for token_id in ....indices],\n",
    "    'probabilities': most_likely_tokens.values...(dim=0),\n",
    "})\n",
    "# Show the table, in a nice formatted way (see https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html#Builtin-Styles)\n",
    "most_likely_tokens_df.style.hide_index().background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write a function that is given a phrase and a *k* and returns the top *k* most likely next tokens.\n",
    "\n",
    "Build this function using only code that you've already filled in above. Clean up the code so that it doesn't do or display anything extraneous. Add comments about what each step does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kJWXqQLLkCyP",
    "outputId": "b969f2ed-2289-48c2-e717-b4802a493c5d"
   },
   "outputs": [],
   "source": [
    "def predict_next_tokens(...):\n",
    "    # your code here\n",
    "\n",
    "check_global_vars(predict_next_tokens, [\"torch\", \"tokenizer\", \"pd\", \"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_299ee_row0_col1 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_299ee_row1_col1 {\n",
       "  background-color: #7dacd1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_299ee_row2_col1 {\n",
       "  background-color: #f4edf6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_299ee_row3_col1 {\n",
       "  background-color: #f9f2f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_299ee_row4_col1 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_299ee_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"col_heading level0 col0\" >tokens</th>\n",
       "      <th class=\"col_heading level0 col1\" >probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_299ee_row0_col0\" class=\"data row0 col0\" > go</td>\n",
       "      <td id=\"T_299ee_row0_col1\" class=\"data row0 col1\" >0.306805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_299ee_row1_col0\" class=\"data row1 col0\" > take</td>\n",
       "      <td id=\"T_299ee_row1_col1\" class=\"data row1 col1\" >0.225022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_299ee_row2_col0\" class=\"data row2 col0\" > spend</td>\n",
       "      <td id=\"T_299ee_row2_col1\" class=\"data row2 col1\" >0.161895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_299ee_row3_col0\" class=\"data row3 col0\" > make</td>\n",
       "      <td id=\"T_299ee_row3_col1\" class=\"data row3 col1\" >0.156505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_299ee_row4_col0\" class=\"data row4 col0\" > do</td>\n",
       "      <td id=\"T_299ee_row4_col1\" class=\"data row4 col1\" >0.149773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x11bf3d490>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_tokens(\"This weekend I plan to\", 5).style.hide_index().background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f6529_row0_col1 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f6529_row1_col1 {\n",
       "  background-color: #fcf4fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f6529_row2_col1 {\n",
       "  background-color: #fef6fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f6529_row3_col1, #T_f6529_row4_col1 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f6529_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"col_heading level0 col0\" >tokens</th>\n",
       "      <th class=\"col_heading level0 col1\" >probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_f6529_row0_col0\" class=\"data row0 col0\" > be</td>\n",
       "      <td id=\"T_f6529_row0_col1\" class=\"data row0 col1\" >0.926792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f6529_row1_col0\" class=\"data row1 col0\" > have</td>\n",
       "      <td id=\"T_f6529_row1_col1\" class=\"data row1 col1\" >0.030508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f6529_row2_col0\" class=\"data row2 col0\" > the</td>\n",
       "      <td id=\"T_f6529_row2_col1\" class=\"data row2 col1\" >0.018525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f6529_row3_col0\" class=\"data row3 col0\" > do</td>\n",
       "      <td id=\"T_f6529_row3_col1\" class=\"data row3 col1\" >0.013536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f6529_row4_col0\" class=\"data row4 col0\" >,</td>\n",
       "      <td id=\"T_f6529_row4_col1\" class=\"data row4 col1\" >0.010639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x10d230700>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_tokens(\"To be or not to\", 5).style.hide_index().background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_97945_row0_col1 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97945_row1_col1 {\n",
       "  background-color: #79abd0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97945_row2_col1 {\n",
       "  background-color: #d0d1e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97945_row3_col1 {\n",
       "  background-color: #e2dfee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97945_row4_col1 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_97945_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"col_heading level0 col0\" >tokens</th>\n",
       "      <th class=\"col_heading level0 col1\" >probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_97945_row0_col0\" class=\"data row0 col0\" > world</td>\n",
       "      <td id=\"T_97945_row0_col1\" class=\"data row0 col1\" >0.384820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_97945_row1_col0\" class=\"data row1 col0\" > Lord</td>\n",
       "      <td id=\"T_97945_row1_col1\" class=\"data row1 col1\" >0.231493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_97945_row2_col0\" class=\"data row2 col0\" > people</td>\n",
       "      <td id=\"T_97945_row2_col1\" class=\"data row2 col1\" >0.160956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_97945_row3_col0\" class=\"data row3 col0\" > earth</td>\n",
       "      <td id=\"T_97945_row3_col1\" class=\"data row3 col1\" >0.136998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_97945_row4_col0\" class=\"data row4 col0\" > children</td>\n",
       "      <td id=\"T_97945_row4_col1\" class=\"data row4 col1\" >0.085733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x11bf3d460>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_tokens(\"For God so loved the\", 5).style.hide_index().background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsI_Tz0ipglx"
   },
   "source": [
    "## Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Explain the shape of `model_output.logits`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: What would be required to generate more than one token? What decisions would you have to make?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOVes22Uvu4sPkNN1/P8/pg",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "013-lm-logits",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
